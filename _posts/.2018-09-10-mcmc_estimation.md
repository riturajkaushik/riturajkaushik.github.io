---
layout: post
title: Approximation of Bayesian posterior with Markov Chain Monte Carlo 
tags: [statistics, tutorial, Bayesian]
---
Bayes' rule is amazing. Ever since I came across Bayes' rule in statistics I have been in love with this simple yet elegant formula in statistics which has application in wide area of research such as AI, health care, economics and what else. Although it looks very simple, in many cases it becomes very complicated to compute it in closed form. This complication arises due to the denominator of the formula which is the marginal probability distribution of the observations. This term is sometimes very hard (or virtually impossible) to compute analytically. To bypass this complexity, several approaches has been proposed to approximate the Bayesian posterior such as Variational inference, Markov Chain Monte Carlo (MCMC) etc . In this informal tutorial we are going to see how we can approximate the Bayes rule using MCMC.

## Recap with an example
Before diving into the algorithm, just for revision let's first see the Bayes' rule with a linear regression example. Let's you are given 10 random values of $$x$$ and their corresponding values of $$y$$ from the quadratic function $$y = 2x^2 - 3x + 1$$. Our goal is to guess the coefficients quadratic function $$w_0x^2 + w_1x + w_2$$ using the given data (observations). This is an easy problem from machine learning perspective. We can simply do a least square regression with gradient descent to find an estimate of the coefficients $$w_0$$, $$w_1$$ and $$w_2$$. However, this type of regression will give you only an fixed value estimate of these coefficients and thus you will have only a fixed function. Say for example, after the gradient descent we get $$w_0 = 1.87$$, $$w_1 = -2.6$$ and $$w_2 = 0.89$$. Clearly, these values are not same as that of the actual function and these coefficient will not represent the actual function for all the possible values of x that were not in the given data set. The quadratic model with these estimated coefficients will predict $$y$$ for any value of $$x$$ with absolute certainty and this might not be very good for many practical purposes. Many times it is better to have an estimate of prediction uncertainty and with the prediction.

How to get an estimate of the prediction uncertainty? One way to deal with this is to have a probability distribution over the coefficients (say a multivariate Gaussian distribution with some mean and covariance) instead of fixed values. Then we can sample some set of $$(w_0, w_1, w_2)$$ from this distribution and then we can compute the $$y$$ values using each set of $$(w_0, w_1, w_2)$$ for any value of $$x$$. Thus we can have an estimate of mean and variance of predicted $$y$$s. The variance will correspond to the uncertainty in the prediction. 

One way to estimate the probability distribution $$P(w_0, w_1, w_2)$$ or simply $$P(W)$$ is Bayesian inference i.e using Bayes' rule. The Bayes' rule can be written as follows:

$$ 
P(W|D) = \frac{P(D|W)P(W)}{\int {P(D|W_i)dW_i}}
$$

where, $$W$$ is the vector of coefficients of out quadratic model and $$D$$ represents the given data.


